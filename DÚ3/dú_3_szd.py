# -*- coding: utf-8 -*-
"""DÚ 3 SZD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19kmHEcJYnqJKjfIEI-bG0DbFosYRkAQk
"""

import random
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

mu, sigma = 0, 0.1
a = -0.5
b = 0.5
n_samples = 20000

signal_data = np.random.normal(mu, sigma, 2*n_samples)



noise_data = np.random.uniform(a, b, 5*n_samples)

combined_data = np.hstack((signal_data, noise_data))

def find_bin_centers(edges): # the function np.histogram produces bin edges. This simply converts edges to centers used for fitting
    return (edges[1:] + edges[:-1]) / 2

fig, ax = plt.subplots()
count, bins, ignored = ax.hist(combined_data, 30, density=False )
centers = find_bin_centers(bins)

def fit_function(centres, A, B):
    return A * np.exp(- (centres - mu)**2 / (2 * sigma**2)) + B

popt, pcov = curve_fit(fit_function, centers, count)
popt
print("Fit parameters (A, B):", popt) #odhad na parametry A a B, ať nemusím procházet přes všechny možný kombinace

ax.plot(centers, fit_function(centers, *popt), color='r')
fig

def likelihood_function(Nk, Dk):
    #Nk je kontent k-tého binu a Dk je hodnota v k-tém binu - pomocí Dk odhadnu střední hodnotu k-tého binu
    #takže vlastně L = ( <N_i>^N_i / N_i! ) * exp(-<N_i>), kde teda střední hodnota N_i odhadnu hodnotou funkce Di, což je ta pravda, kterou neznám a hledám
    # nejvyšší likilehood bude pro hodnotu Di nejblíž k pravdě, takže postudpně počítám likelihood pro různé Di a hledám jeho maximum
    return Nk*np.log(Dk)-Dk

"""# něco

Di je reálná hodnota třeba účinný průřez * luminosita - počet eventů je to reálný číslo, třeba 12,6

ale já nemůžu naměřit reálný číslo eventů a reálně naměřím nějaký celý číslo eventů - cože je Ni

kde teda Ni bude dáno poissonem, protože je binární - buďto se něco stane nebo nestane a děje se to s nějakou frekvencí, akorát ta střední hodnota bude kolem reálnýho počtu eventů, což je Di

Pak teda likelihood L, resp ln(L) = ∑ln(1\Ni!) - to je cte, nezajímá nás - + ln(střední hodnota Ni = Di na Ni) + ln(exp(-střední hodnota Ni = Di))
"""

def ln_likelihood():
    likelihoodies = []
    max = -np.inf
    min = np.inf
    min_A = 0
    min_B = 0
    max_A = 0
    max_B = 0

    for A in range (5345, 5350):
      for B in range (3300, 3330):
        suma = 0
        for index,center,bin_count in zip(range(len(bins)),centers,count):
          Dk = fit_function(center, A, B)
          Nk = bin_count
          #print(f"A je {A}\nB je {B}")
          #print(Dk, Nk)
          like_výsledek = likelihood_function(Nk, Dk)
          suma += like_výsledek
          #print(f"suma je {suma}, přidal jsem k ní {like_výsledek}")
        if suma < min:
          min = suma
          min_A = A
          min_B = B
        if suma > max:
          max = suma
          max_A = A
          max_B = B
        likelihoodies.append(suma)
    return likelihoodies, max, min, max_A, max_B, min_A, min_B

likelihoodies, max, min, max_A, max_B, min_A, min_B = ln_likelihood()
print(f"maximum je {max} pro A a B {max_A}, {max_B}")

best_A, best_B = max_A, max_B
ax.plot(centers, fit_function(centers, *popt), color='r')

ax.axhline(y = best_B, color = 'b', alpha=0.6) # dělání čar, ať vidím, že to nevrátilo hovadiny
ax.axhline(y = best_A + best_B, color = 'aqua', alpha=0.6)
print(sum(count))
fig

best_A, best_B

"""#rychlejší a asi i přesnější způsob od chatu"""

from scipy.optimize import minimize

# Define the log-likelihood function
def log_likelihood(params):
    A, B = params  # Unpack parameters
    suma = 0
    for center, bin_count in zip(centers, count):
        D_k = fit_function(center, A, B)
        if D_k > 0:  # Avoid log(0) issues
            suma += bin_count * np.log(D_k) - D_k
    return -suma  # Negative because we want to maximize

# Initial guesses for A and B
initial_guess = [4300, 1700]

# Minimize the negative log-likelihood
result = minimize(log_likelihood, initial_guess, bounds=[(200, 10000), (105, 5000)])

 #odkomentit, jestli chce člověk použít výsledky z tohto
best_A, best_B = result.x
print(f"Best A: {best_A}, Best B: {best_B}")

ratio = best_A/best_B

def compute_noise(B, x_min, x_max):
    return B * len(bins)

N = 100000
def monte_carlo_gaus(x_i):
    return  np.exp(-(x_i - mu)**2 / (2 * sigma**2))
uniform_sample = np.random.uniform(a, b, N)

I = 0
for i in range(len(uniform_sample)):
    I += monte_carlo_gaus(uniform_sample[i])

I_signal =best_A* ((b-a)/N) * I *len(bins)






I_B = compute_noise(best_B, a,b)


print(f"Number of Signal Events (I_signal): {I_signal}")
print(f"Number of Noise Events (I_B): {I_B}")
print(f"Total Events (I_signal + I_B): {I_signal + I_B}")
print(f"původní počet eventů {len(combined_data)}")

"""za speciálního příklad, kdy mám data body nějak "rozmazaný" pomocí gausse, tzn každý bin fluktuuje s nějakou amplitudou sigma_i, tak můžu použít jinou speciální verzi maximum likelihood, kde teda mám likelihood ne jenom nějakou naměřenou hodnotu mínus reálnou hodnotu, ale ještě to podělím tou chybou jednotlivých binů sigma_i a to celý dám na kvadrát, abych dostal nějaký "vážený likelihood" - pak suma těhle vážených likelihoodů je distrubuuovaná podle chí kvadrátu teda, kdybych měření prováděl znova a znova tak mi ty Vážený likelihoody vyjdou distribuovaný podle chí kvadrát

"""

